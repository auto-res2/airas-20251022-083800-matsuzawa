
LLM Name: o3-2025-04-16
Input:
You are an AI code reviewer validating production-ready experiment code for research papers.

Analyze the provided experiment code and determine if it meets all requirements for immediate execution in research experiments.

# Instructions

## Core Validation Criteria
Check if the generated experiment code meets ALL of the following requirements:

1. **Complete Implementation**:
   - Every component is fully functional, production-ready, publication-worthy code
   - No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
   - All functions and classes are completely implemented
   - No truncated code sections or incomplete implementations

2. **Hydra Integration**:
   - Uses Hydra to manage all experiment configurations from `config/run/*.yaml` files
   - All parameters are loaded from run configs dynamically
   - Proper configuration structure with run_id, method, model, dataset, training, and optuna sections
   - CLI interface matches:
     * Training: `uv run python -u -m src.main run={run_id} results_dir={path}`
     * Evaluation: `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'` (independent execution)
   - Supports trial_mode=true flag for lightweight validation runs (automatically disables WandB)

3. **Complete Data Pipeline**:
   - Full data loading and preprocessing implementation
   - Dataset-specific preprocessing is properly implemented
   - No placeholder dataset loading code
   - Proper error handling for data operations
   - Uses `.cache/` as the cache directory for all datasets and models

4. **Model Implementation**:
   - Complete model architectures for all methods (proposed and comparative methods)
   - No placeholders (TODO, PLACEHOLDER, pass, or incomplete implementations)
   - When External Resources specify HuggingFace models: properly use and customize them (acceptable to wrap AutoModel, add adapters, etc.)
   - When no external models specified: implement architectures from scratch using PyTorch primitives
   - Model-specific configurations correctly applied
   - Proper PyTorch usage throughout

5. **File Structure Compliance**:
   - Contains EXACTLY these required files (and NO other files):
     * `src/train.py`
     * `src/evaluate.py`
     * `src/preprocess.py`
     * `src/model.py`
     * `src/main.py`
     * `pyproject.toml`
     * `config/config.yaml`
   - NO additional files (e.g., NO `src/__init__.py`, NO `setup.py`, NO other Python files)
   - No missing files from the structure
   - All functionality contained within specified files

6. **WandB Integration**:
   - train.py initializes WandB and logs ALL metrics comprehensively:
     * Use `wandb.log()` at each training step/batch/epoch with ALL relevant time-series metrics
     * Log as frequently as possible (per-batch or per-epoch) to capture complete training dynamics
     * Use `wandb.summary["key"] = value` to save final/best metrics (best_val_acc, final_test_acc, best_epoch, etc.)
   - trial_mode automatically disables WandB (sets wandb.mode=disabled)
   - NO results.json or stdout JSON dumps in train.py
   - config/config.yaml contains mandatory WandB settings (entity/project)

7. **Configuration Files**:
   - The generated code properly references config files via Hydra
   - NOTE: config/run/{run_id}.yaml files are provided separately (not in ExperimentCode)
   - All run configurations match the experiment_runs provided
   - Optuna search spaces are properly defined if applicable

8. **Evaluation Script Independence**:
   - evaluate.py is executed independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
   - Accepts `run_ids` parameter as JSON string list (parse with `json.loads(args.run_ids)`)
   - main.py DOES NOT call evaluate.py
   - evaluate.py retrieves comprehensive data from WandB API:
     * Use `wandb.Api()` to get run data: `run = api.run(f"{entity}/{project}/{run_id}")`
     * Retrieve: `history = run.history()`, `summary = run.summary._json_dict`, `config = dict(run.config)`
   - **STEP 1: Per-Run Processing** (for each run_id):
     * Export comprehensive run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
     * Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
     * Each run should have its own subdirectory with its metrics and figures
   - **STEP 2: Aggregated Analysis** (after processing all runs):
     * Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
     * Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
     * Generate comparison figures to: `{results_dir}/comparison/`
     * Cross-run comparison charts (bar charts, box plots)
     * Performance metrics tables
     * Statistical significance tests
   - Proper figure quality: legends, annotations, tight_layout
   - Follows GLOBALLY UNIQUE naming convention to prevent collisions:
     * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
     * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
   - train.py and main.py generate NO figures
   - evaluate.py cannot run in trial_mode (no WandB data available when WandB disabled)

9. **Trial Mode Implementation**:
   - trial_mode=true flag properly reduces computational load
   - Training: epochs=1, batches limited to 1-2, Optuna disabled (n_trials=0), small evaluation subset
   - WandB automatically disabled in trial_mode (wandb.mode=disabled)
   - Purpose: Fast validation that code runs without errors

## Output Format
Respond with a JSON object containing:
- `is_code_ready`: boolean - true if ALL criteria are met, false otherwise
- `code_issue`: string - specific issues found if any criteria are not met, focusing on what needs to be fixed

# Current Research Method
{
    "Open Problems": "1. BOIL estimates the wall-clock cost of evaluating a hyper-parameter setting with a plain LinearRegression model.  In practice the relation (e.g. GPU time, number of simulator steps) is highly non-linear and heteroscedastic.\n2. The acquisition function divides the expected utility by the point estimate of the cost, ignoring its uncertainty; when the cost model is wrong the optimiser can select apparently cheap but actually expensive configurations, wasting budget.\n3. Both shortcomings can be fixed with a very small change – replacing the linear cost surrogate with a probabilistic model and letting the acquisition treat cost uncertainty explicitly.",
    "Methods": "Method name: CA-BOIL (Cost-Aware BOIL)\nMinimal changes to BOIL:\n1. Replace the LinearRegression cost model with a second Gaussian-Process surrogate GP_c(x,t) that yields mean µ_c and standard deviation σ_c of the (log) cost.\n2. Redefine the acquisition used in utility_cost_evaluation from\n        U_old =  log(EI) – log(µ_c)\n   to the cost-uncertainty–aware\n        U_new =  EI /( µ_c + κ · σ_c )\n   (equivalently  log(EI) – log(µ_c+κσ_c) in BOIL’s code).  κ≥0 is a single scalar (default κ=1).\nTheoretical intuition:\n• GP_c gives a calibrated posterior over cost.  Dividing by µ_c+κσ_c is the familiar concept of \"expected improvement per (risk-adjusted) cost\"; high uncertainty on cost is penalised so the optimiser prefers configurations that are both promising and reliably cheap.\n• The change keeps BOIL’s product-kernel GP for utility, the learning-curve compression, the data-augmentation, etc. completely intact – only 25 lines of code are touched.",
    "Experimental Setup": "Goal: show that CA-BOIL reaches the same test accuracy with less wall-clock time than vanilla BOIL.\nTask: tune (learning-rate, hidden-units) of an MLP on the Fashion-MNIST dataset for 10 epochs.\nTrue cost: measured GPU time per evaluation (simulated here as  0.4·hidden_units + 1200·learning_rate^2  + noise).\nProtocol:\n1. Use the public BOIL implementation as the baseline (n_init_points=3, budget 40 evaluations).\n2. Implement CA-BOIL by adding GP_c and U_new (κ=1).\n3. Run each optimiser 20 times with different random seeds.\nMetrics:\n• Best validation accuracy reached vs. cumulative true time (area under curve).\n• Total time to reach 85% accuracy.",
    "Experimental Code": "import numpy as np, time, torch, sklearn\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel\n\n# === tiny wrapper around the user objective =================================\n\ndef objective(cfg):\n    lr, h = cfg  # learning-rate, hidden units\n    start = time.time()\n    # --- train a very small network for 10 epochs (placeholder) ------------\n    acc = 1.0 - (lr*3 + (200-h)/400) + np.random.randn()*0.01  # fake accuracy\n    true_cost = 0.4*h + 1200*lr**2 + np.random.rand()*3        # fake time\n    return acc, np.log(true_cost), time.time()-start\n\n# === cost-aware acquisition =================================================\n\ndef expected_improvement(mu, sigma, y_best):\n    from scipy.stats import norm\n    z = (mu - y_best)/sigma\n    return (mu - y_best)*norm.cdf(z) + sigma*norm.pdf(z)\n\ndef acquisition(X, gp_u, gp_c, y_best, kappa):\n    mu_u, sigma_u = gp_u.predict(X, return_std=True)\n    mu_c, sigma_c = gp_c.predict(X, return_std=True)\n    ei = expected_improvement(mu_u, sigma_u, y_best)\n    return ei / (mu_c + kappa*sigma_c)\n\n# === minimal CA-BOIL optimisation loop (no virtual obs for brevity) =========\n\nbounds = np.array([[1e-4, 1e-1],   # learning rate\n                   [ 50 , 400 ]])  # hidden units\n\ngp_u = GaussianProcessRegressor(Matern()+WhiteKernel(1e-5))\ngp_c = GaussianProcessRegressor(Matern()+WhiteKernel(1e-5))\n\nX, y_u, y_c = [], [], []\n# initial design\nfor _ in range(3):\n    x = np.random.uniform(bounds[:,0], bounds[:,1])\n    acc, cost_log, _ = objective(x)\n    X.append(x)\n    y_u.append(acc)\n    y_c.append(cost_log)\n\nfor it in range(37):  # total 40\n    gp_u.fit(np.array(X), np.array(y_u))\n    gp_c.fit(np.array(X), np.array(y_c))\n\n    y_best = np.max(y_u)\n    # optimise acquisition by random search (sufficient for demo)\n    cand = np.random.uniform(bounds[:,0], bounds[:,1], size=(5000,2))\n    acq_values = acquisition(cand, gp_u, gp_c, y_best, kappa=1.0)\n    x_next = cand[np.argmax(acq_values)]\n\n    acc, cost_log, _ = objective(x_next)\n    X.append(x_next)\n    y_u.append(acc)\n    y_c.append(cost_log)\n\nprint('best val acc', np.max(y_u))\n",
    "Expected Result": "Across 20 independent runs CA-BOIL reduces the median wall-clock time to reach 85% accuracy by ≈20-30% compared with BOIL, and achieves slightly higher final accuracy for the same fixed time budget because it avoids a few very costly evaluations that BOIL selects due to under-estimated cost.",
    "Expected Conclusion": "By swapping the linear cost surrogate for a simple GP and dividing the improvement by a risk-adjusted cost estimate, CA-BOIL becomes less prone to costly mistakes while keeping BOIL’s strengths (learning-curve compression and data augmentation).  The single added κ hyper-parameter and ~25 lines of code yield a consistent speed-up, demonstrating that even very small, principled modifications can translate into tangible efficiency gains in hyper-parameter optimisation."
}

# Experimental Design
- Strategy: We benchmark the new Cost-Aware BOIL (CA-BOIL) hyper-parameter optimiser against the original BOIL on a medium-size neural-network tuning task.  The optimiser must find the learning-rate (10⁻⁴–10⁻¹) and hidden-layer width (50–400 units) that maximise validation accuracy of a 2-layer MLP after ten training epochs on Fashion-MNIST while paying the true wall-clock cost of each trial (measured GPU time).  
Workflow
1. Dataset loading: Fashion-MNIST train/validation split.
2. Model definition: a fixed 2-layer MLP (≈0.1 M parameters) implemented in PyTorch.
3. Objective function: (cfg) ↦ (validation-accuracy, log(cost)).  Cost is obtained through torch.cuda.Event timers.
4. Optimisers:
   • BOIL baseline: single GP for utility, linear regression for cost, acquisition U_old = log(EI) – log(µ_c).
   • Proposed CA-BOIL: identical utility GP but replaces the linear cost model with a GP GP_c and uses acquisition U_new = EI / (µ_c + κ σ_c) (κ = 1).
5. Loop: 3 Sobol initial points + 37 adaptive steps (budget 40).  Both methods are repeated for 20 random seeds on the same machine (8 × A100, 80 GB each).
6. Logging: after every evaluation we record cumulative wall-clock time, best validation accuracy, chosen hyper-parameters, and acquisition value.
7. Post-processing: compute per-run curves best-acc(t), integrate AUC, and measure the time when 85 % validation accuracy is first reached.
8. Statistical analysis: report median and 95 %-CI across seeds, and perform two-sided Wilcoxon signed-rank tests on each metric.
The experiment demonstrates that CA-BOIL reaches 85 % accuracy ≈25 % faster and avoids extreme outliers in time consumption, validating that modelling cost uncertainty yields more budget-robust optimisation.
- Proposed Method: Cost-Aware BOIL (CA-BOIL)
Objective: accelerate Bayesian Optimisation with Inequality Learning (BOIL) by taking uncertainty in evaluation cost into account.
Key ideas:
1. Probabilistic cost surrogate: replace BOIL’s linear regression on observed (log) costs with a Gaussian-Process surrogate GP_c(x,t) with Matern + white kernels.  The GP yields a posterior mean µ_c(x) and standard deviation σ_c(x).
2. Risk-adjusted acquisition: for a candidate configuration x the utility acquisition is defined as
      U_new(x) = EI(x) / ( µ_c(x) + κ σ_c(x) ),
   where EI is the standard Expected Improvement of the utility GP and κ≥0 (default 1) controls cost-risk aversion.
3. Implementation: only ~25 lines change in the open-source BOIL codebase—add a second GP object, train it on the same design points’ logged costs, and swap one line in utility_cost_evaluation to compute U_new.
4. Algorithmic steps per BO iteration:
   a. Fit GP_u on (x, validation_accuracy).
   b. Fit GP_c on (x, log(cost)).
   c. Maximise U_new(x) over the search space via Thompson sampling + local L-BFGS.
   d. Evaluate the objective at x*, append the new data, repeat until budget.
Theoretically, dividing EI by a risk-adjusted denominator is equivalent to optimising expected improvement per unit of cost under a mean-variance formulation, so candidates with high cost uncertainty are automatically down-weighted.  The method preserves every other aspect of BOIL (learning-curve compression, virtual observations, product kernels) and introduces only one additional scalar hyper-parameter κ.
- Evaluation Metrics: ['Area Under Best-Accuracy–Time Curve', 'Time To 85% Accuracy', 'Final Best Validation Accuracy']

# Experiment Runs

- Run ID: proposed-2-Layer-MLP-0.1M-parameters--Fashion-MNIST
  Method: proposed
  Model: 2-Layer MLP (≈0.1M parameters)
  Dataset: Fashion-MNIST
  
  Config Content:
  ```yaml
  run_id: proposed-2-Layer-MLP-0.1M-parameters--Fashion-MNIST
method: proposed
algorithm:
  name: CA-BOIL
  n_init_points: 3         # Sobol seed evaluations
  total_evaluations: 40    # 3 initial + 37 BO steps
  cost_model:
    type: gp
    kernel: matern_plus_white
  acquisition:
    type: ei_over_risk_adjusted_cost
    kappa: 1.0             # default; can be tuned by Optuna
    formula: "EI / (mu_c + kappa * sigma_c)"
model:
  name: 2-layer-mlp
  input_dim: 784           # 28×28 images flattened
  hidden_units: 200        # placeholder, will be overridden by Optuna suggestion
  activation: relu
  output_dim: 10
  parameter_budget: "≈0.1M"
dataset:
  name: fashion_mnist
  root: ./data
  train_size: 50000        # 83 % of full train split
  val_size: 10000          # 17 % of full train split
  normalization: true
training:
  epochs: 10               # fixed according to protocol
  batch_size: 128
  optimizer: adam
  weight_decay: 0.0
  max_grad_norm: 1.0
  device: cuda
logging:
  use_wandb: true
  project: ca-boil
  log_interval: 1          # log after each evaluation
resources:
  gpus_per_trial: 1
  gpu_type: A100
  cpus_per_trial: 4
  memory_gb: 16
optuna:
  n_trials: 50
  direction: maximize
  sampler: tpe
  pruner: median
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    hidden_units:
      type: int
      low: 50
      high: 400
      step: 1
    kappa:
      type: uniform
      low: 0.5
      high: 2.0

  ```
  

- Run ID: comparative-1-2-Layer-MLP-0.1M-parameters--Fashion-MNIST
  Method: comparative-1
  Model: 2-Layer MLP (≈0.1M parameters)
  Dataset: Fashion-MNIST
  
  Config Content:
  ```yaml
  run_id: comparative-1-2-Layer-MLP-0.1M-parameters--Fashion-MNIST
method: comparative-1
algorithm:
  name: BOIL
  n_init_points: 3
  total_evaluations: 40
  cost_model:
    type: linear           # original BOIL uses plain LinearRegression
  acquisition:
    type: log_ei_minus_log_cost
    formula: "log(EI) - log(mu_c)"
model:
  name: 2-layer-mlp
  input_dim: 784
  hidden_units: 200        # placeholder, tuned by Optuna
  activation: relu
  output_dim: 10
  parameter_budget: "≈0.1M"
dataset:
  name: fashion_mnist
  root: ./data
  train_size: 50000
  val_size: 10000
  normalization: true
training:
  epochs: 10
  batch_size: 128
  optimizer: adam
  weight_decay: 0.0
  max_grad_norm: 1.0
  device: cuda
logging:
  use_wandb: true
  project: boil-baseline
  log_interval: 1
resources:
  gpus_per_trial: 1
  gpu_type: A100
  cpus_per_trial: 4
  memory_gb: 16
optuna:
  n_trials: 50
  direction: maximize
  sampler: tpe
  pruner: median
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    hidden_units:
      type: int
      low: 50
      high: 400
      step: 1

  ```
  


# Generated Experiment Code (To be validated)
{"config_yaml": "defaults:\n  - _self_\n  - run: ???\n\n# --- general flags -----------------------------------------------------------\nrun:\n  run_id: null\n\nresults_dir: ./results\ntrial_mode: false\n\n# --- wandb -------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 251021-test\n  mode: online\n\n# --- placeholders that run-configs overwrite ---------------------------------\nmethod: \"\"\nalgorithm:\n  name: \"\"\n  n_init_points: 3\n  total_evaluations: 40\n  cost_model: {}\n  acquisition: {}\nmodel: {}\ndataset: {}\ntraining:\n  device: cuda\n  epochs: 10\n  batch_size: 128\n  optimizer: adam\n  weight_decay: 0.0\n  max_grad_norm: 1.0\n\noptuna:\n  n_trials: 50\n  direction: maximize\n  sampler: tpe\n  pruner: median\n  search_space: {}\n", "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom scipy.stats import wilcoxon\n\n\n# --------------------------------------------------------------------------------------\n# Helper utilities\n# --------------------------------------------------------------------------------------\n\ndef _ensure_dir(p: Path):\n    p.mkdir(parents=True, exist_ok=True)\n    return p\n\n\ndef _plot_learning_curve(history_df: pd.DataFrame, run_id: str, out_path: Path):\n    plt.figure(figsize=(6, 4))\n    if \"cumulative_time\" in history_df.columns:\n        x = history_df[\"cumulative_time\"]\n    else:\n        x = history_df.index\n    y = history_df[\"best_so_far\"] if \"best_so_far\" in history_df.columns else history_df[\"val_accuracy\"]\n    plt.plot(x, y, label=\"Validation Accuracy\")\n    plt.xlabel(\"Cumulative Time (s)\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(f\"Learning Curve {run_id}\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _plot_confusion_matrix(cm: np.ndarray, run_id: str, out_path: Path):\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"Confusion Matrix {run_id}\")\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _aggregate_and_compare(results, out_dir: Path):\n    records = []\n    for run_id, data in results.items():\n        summary = data[\"summary\"]\n        records.append({\n            \"run_id\": run_id,\n            \"best_val_acc\": summary.get(\"final_best_val_acc\", float(\"nan\")),\n            \"auc\": summary.get(\"auc_accuracy\", float(\"nan\")),\n            \"time_to_85\": summary.get(\"time_to_85\", float(\"nan\")),\n        })\n    df = pd.DataFrame.from_records(records)\n    _ensure_dir(out_dir)\n    df.to_json(out_dir / \"aggregated_metrics.json\", orient=\"records\", lines=False, indent=2)\n\n    # Bar chart of final accuracy\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=\"best_val_acc\", data=df)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Best Validation Accuracy\")\n    for i, v in enumerate(df[\"best_val_acc\"]):\n        plt.text(i, v + 0.001, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    plt.tight_layout()\n    bar_path = out_dir / \"comparison_best_accuracy_bar_chart.pdf\"\n    plt.savefig(bar_path, format=\"pdf\")\n    plt.close()\n\n    # Statistical significance (wilcoxon) if we have pairs\n    if len(df) == 2:\n        try:\n            stat, p = wilcoxon(df.loc[0, \"best_val_acc\"], df.loc[1, \"best_val_acc\"])\n        except Exception:\n            p = float(\"nan\")\n        with open(out_dir / \"statistics.txt\", \"w\") as f:\n            f.write(f\"Wilcoxon p-value on best_val_acc: {p}\\n\")\n    print(f\"Generated aggregated metrics and comparison figure at {out_dir}\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON string list of run IDs\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    run_ids = json.loads(args.run_ids)\n\n    api = wandb.Api()\n\n    all_results = {}\n\n    for run_id in run_ids:\n        run_save_dir = _ensure_dir(results_dir / run_id)\n\n        # Retrieve run\n        run = api.run(f\"{os.environ.get(\u0027WANDB_ENTITY\u0027, \u0027gengaru617-personal\u0027)}/{os.environ.get(\u0027WANDB_PROJECT\u0027, \u0027251021-test\u0027)}/{run_id}\")\n        history = run.history()  # pandas DataFrame\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        # Export metrics\n        metrics_path = run_save_dir / \"metrics.json\"\n        history.to_json(metrics_path, orient=\"records\", lines=False, indent=2)\n        print(metrics_path)\n\n        # Learning curve figure\n        lc_path = run_save_dir / f\"{run_id}_learning_curve.pdf\"\n        _plot_learning_curve(history, run_id, lc_path)\n        print(lc_path)\n\n        # Confusion matrix figure (if available)\n        if \"confusion_matrix\" in summary:\n            cm_array = np.array(summary[\"confusion_matrix\"])\n            cm_path = run_save_dir / f\"{run_id}_confusion_matrix.pdf\"\n            _plot_confusion_matrix(cm_array, run_id, cm_path)\n            print(cm_path)\n\n        all_results[run_id] = {\n            \"history\": history,\n            \"summary\": summary,\n            \"config\": config,\n        }\n\n    # Aggregated analysis\n    comparison_dir = results_dir / \"comparison\"\n    _aggregate_and_compare(all_results, comparison_dir)\n    print(comparison_dir / \"aggregated_metrics.json\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "main_py": "import subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    \"\"\"Orchestrator that spawns src.train as a subprocess.\"\"\"\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run.run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n    ]\n\n    # propagate trial_mode\n    if cfg.trial_mode:\n        cmd.append(\"trial_mode=true\")\n        cmd.append(\"wandb.mode=disabled\")\n\n    print(\"Executing:\", \" \".join(cmd))\n\n    proc = subprocess.Popen(cmd)\n    proc.communicate()\n    if proc.returncode != 0:\n        raise SystemExit(proc.returncode)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model_py": "import torch.nn as nn\n\n\nclass MultiLayerPerceptron(nn.Module):\n    def __init__(self, input_dim: int, hidden_units: int, output_dim: int, activation: str = \"relu\"):\n        super().__init__()\n        if activation == \"relu\":\n            act = nn.ReLU()\n        elif activation == \"tanh\":\n            act = nn.Tanh()\n        else:\n            raise ValueError(f\"Unsupported activation {activation}\")\n\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(input_dim, hidden_units),\n            act,\n            nn.Linear(hidden_units, output_dim),\n        )\n\n    def forward(self, x):  # type: ignore\n        return self.net(x)\n", "preprocess_py": "from typing import Tuple\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import FashionMNIST\n\n\nCACHE_DIR = Path(\".cache\")\n\n\ndef get_dataloaders(cfg) -\u003e Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"Return train, val, test dataloaders as specified in cfg.dataset.\"\"\"\n    root = Path(cfg.dataset.root) if hasattr(cfg.dataset, \"root\") else CACHE_DIR / \"fashion_mnist\"\n    root.mkdir(parents=True, exist_ok=True)\n\n    transform_list = [transforms.ToTensor()]\n    if cfg.dataset.normalization:\n        transform_list.append(transforms.Normalize((0.5,), (0.5,)))\n    transform = transforms.Compose(transform_list)\n\n    full_train = FashionMNIST(root=str(root), train=True, transform=transform, download=True)\n    test_set = FashionMNIST(root=str(root), train=False, transform=transform, download=True)\n\n    train_size = cfg.dataset.train_size\n    val_size = cfg.dataset.val_size\n    assert train_size + val_size \u003c= len(full_train), \"Train/val sizes exceed dataset length\"\n\n    train_subset, val_subset = random_split(full_train, [train_size, val_size])\n\n    batch_size = cfg.training.batch_size\n\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    return train_loader, val_loader, test_loader\n", "pyproject_toml": "[tool.poetry]\nname = \"ca-boil-experiments\"\nversion = \"0.1.0\"\ndescription = \"Cost-Aware BOIL experimental code\"\nauthors = [\"Researcher \u003cresearch@example.com\u003e\"]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.16.0\"\ntorch = \"^2.1.0\"\ntorchvision = \"^0.16.0\"\nscikit-learn = \"^1.3.2\"\noptuna = \"^3.4.0\"\nnumpy = \"^1.26.2\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\npandas = \"^2.1.3\"\nscipy = \"^1.11.3\"\ntqdm = \"^4.66.1\"\nomegaconf = \"^2.3.0\"\n", "train_py": "import os\nimport sys\nimport time\nimport json\nimport math\nimport random\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel\nfrom sklearn.linear_model import LinearRegression\nimport wandb\nfrom tqdm import tqdm\n\nfrom src.preprocess import get_dataloaders\nfrom src.model import MultiLayerPerceptron\n\n# --------------------------------------------------------------------------------------\n# Helper functions\n# --------------------------------------------------------------------------------------\n\ndef _make_device(cfg):\n    if torch.cuda.is_available() and cfg.training.device == \"cuda\":\n        return torch.device(\"cuda\")\n    return torch.device(\"cpu\")\n\n\ndef _time_now():\n    return time.perf_counter()\n\n\ndef _measure_runtime(fn):\n    \"\"\"Decorator to measure wall-clock time of fn.\"\"\"\n    def wrapper(*args, **kwargs):\n        start = _time_now()\n        out = fn(*args, **kwargs)\n        cost = _time_now() - start\n        return out, cost\n    return wrapper\n\n\n@_measure_runtime\ndef _train_one_setting(lr: float, hidden_units: int, cfg: DictConfig, loaders, device):\n    train_loader, val_loader = loaders\n    model = MultiLayerPerceptron(\n        input_dim=cfg.model.input_dim,\n        hidden_units=hidden_units,\n        output_dim=cfg.model.output_dim,\n        activation=cfg.model.activation,\n    ).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=cfg.training.weight_decay)\n\n    best_acc = 0.0\n    model.train()\n    for epoch in range(cfg.training.epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            if cfg.trial_mode and batch_idx \u003e= 2:\n                break  # drastically shorten trial mode\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n        # validation after each epoch (simple)\n        val_acc = _evaluate(model, val_loader, device, cfg.trial_mode)\n        best_acc = max(best_acc, val_acc)\n    # confusion matrix at the end using best model (last epoch here)\n    cm = _confusion_matrix(model, val_loader, device, cfg.model.output_dim, cfg.trial_mode)\n    return {\n        \"best_val_accuracy\": best_acc,\n        \"confusion_matrix\": cm,\n        \"final_model_state_dict\": model.state_dict(),\n    }\n\n\ndef _evaluate(model, loader, device, trial_mode=False):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(loader):\n            if trial_mode and batch_idx \u003e= 2:\n                break\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    return correct / total if total \u003e 0 else 0.0\n\n\ndef _confusion_matrix(model, loader, device, num_classes, trial_mode=False):\n    all_pred, all_true = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(loader):\n            if trial_mode and batch_idx \u003e= 5:\n                break\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            _, preds = outputs.max(1)\n            all_pred.extend(preds.cpu().numpy())\n            all_true.extend(targets.numpy())\n    cm = confusion_matrix(all_true, all_pred, labels=list(range(num_classes)))\n    return cm.tolist()\n\n\n# --------------------------------------------------------------------------------------\n# BOIL baseline\n# --------------------------------------------------------------------------------------\n\ndef run_baseline_boil(cfg, loaders, device):\n    bounds = np.array([[math.log10(cfg.optuna.search_space.learning_rate.low),\n                        math.log10(cfg.optuna.search_space.learning_rate.high)],\n                       [cfg.optuna.search_space.hidden_units.low,\n                        cfg.optuna.search_space.hidden_units.high]])\n\n    def _sample_random(n):\n        lr = 10 ** np.random.uniform(bounds[0, 0], bounds[0, 1], size=n)\n        hidden = np.random.randint(bounds[1, 0], bounds[1, 1] + 1, size=n)\n        return np.stack([lr, hidden], axis=1)\n\n    n_init = cfg.algorithm.n_init_points\n    total_evals = cfg.algorithm.total_evaluations if not cfg.trial_mode else 1\n\n    X, y_u, y_c = [], [], []\n    cumulative_time = 0.0\n    best_so_far = 0.0\n\n    cost_model = LinearRegression()\n    utility_gp = GaussianProcessRegressor(kernel=Matern() + WhiteKernel(1e-5))\n\n    for evaluation in range(total_evals):\n        if evaluation \u003c n_init:\n            x_next = _sample_random(1)[0]\n        else:\n            # fit models\n            utility_gp.fit(np.array(X), np.array(y_u))\n            cost_model.fit(np.array(X), np.array(y_c))\n            mu_c = cost_model.predict\n\n            # random search for acquisition\n            cand = _sample_random(4000)\n            mu_u, sigma_u = utility_gp.predict(cand, return_std=True)\n            cost_pred = mu_c(cand)\n            ei = _expected_improvement(mu_u, sigma_u, max(y_u))\n            acq = np.log(ei + 1e-12) - np.log(np.exp(cost_pred))  # since cost is log-cost\n            x_next = cand[np.argmax(acq)]\n\n        # evaluate objective\n        lr, hidden = float(x_next[0]), int(round(x_next[1]))\n        res, wall = _train_one_setting(lr, hidden, cfg, loaders, device)\n        val_acc = res[\"best_val_accuracy\"]\n\n        cumulative_time += wall\n        X.append(x_next)\n        y_u.append(val_acc)\n        y_c.append(np.log(wall))  # use log-cost as in BOIL paper\n        best_so_far = max(best_so_far, val_acc)\n\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log({\n                \"evaluation\": evaluation,\n                \"lr\": lr,\n                \"hidden_units\": hidden,\n                \"val_accuracy\": val_acc,\n                \"best_so_far\": best_so_far,\n                \"cumulative_time\": cumulative_time,\n                \"wall_clock\": wall,\n            })\n\n    # summary\n    auc = _area_under_curve(np.arange(len(y_u)), y_u)\n    time_to_85 = _first_time_reach(np.array(y_u), np.array([cumulative_time]))\n    if cfg.wandb.mode != \"disabled\":\n        wandb.summary[\"final_best_val_acc\"] = best_so_far\n        wandb.summary[\"auc_accuracy\"] = auc\n        wandb.summary[\"time_to_85\"] = time_to_85\n\n\n# --------------------------------------------------------------------------------------\n# CA-BOIL proposed method\n# --------------------------------------------------------------------------------------\n\ndef run_ca_boil(cfg, loaders, device):\n    bounds = np.array([[math.log10(cfg.optuna.search_space.learning_rate.low),\n                        math.log10(cfg.optuna.search_space.learning_rate.high)],\n                       [cfg.optuna.search_space.hidden_units.low,\n                        cfg.optuna.search_space.hidden_units.high]])\n\n    def _sample_random(n):\n        lr = 10 ** np.random.uniform(bounds[0, 0], bounds[0, 1], size=n)\n        hidden = np.random.randint(bounds[1, 0], bounds[1, 1] + 1, size=n)\n        return np.stack([lr, hidden], axis=1)\n\n    n_init = cfg.algorithm.n_init_points\n    total_evals = cfg.algorithm.total_evaluations if not cfg.trial_mode else 1\n    kappa = cfg.algorithm.acquisition.kappa\n\n    X, y_u, y_c = [], [], []\n    cumulative_time = 0.0\n    best_so_far = 0.0\n\n    utility_gp = GaussianProcessRegressor(kernel=Matern() + WhiteKernel(1e-5))\n    cost_gp = GaussianProcessRegressor(kernel=Matern() + WhiteKernel(1e-5))\n\n    for evaluation in range(total_evals):\n        if evaluation \u003c n_init:\n            x_next = _sample_random(1)[0]\n        else:\n            utility_gp.fit(np.array(X), np.array(y_u))\n            cost_gp.fit(np.array(X), np.array(y_c))\n\n            cand = _sample_random(4000)\n            mu_u, sigma_u = utility_gp.predict(cand, return_std=True)\n            mu_c, sigma_c = cost_gp.predict(cand, return_std=True)\n\n            ei = _expected_improvement(mu_u, sigma_u, max(y_u))\n            denom = mu_c + kappa * sigma_c\n            acq = ei / (denom + 1e-9)\n            x_next = cand[np.argmax(acq)]\n\n        lr, hidden = float(x_next[0]), int(round(x_next[1]))\n        res, wall = _train_one_setting(lr, hidden, cfg, loaders, device)\n        val_acc = res[\"best_val_accuracy\"]\n\n        cumulative_time += wall\n        X.append(x_next)\n        y_u.append(val_acc)\n        y_c.append(np.log(wall))\n        best_so_far = max(best_so_far, val_acc)\n\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log({\n                \"evaluation\": evaluation,\n                \"lr\": lr,\n                \"hidden_units\": hidden,\n                \"val_accuracy\": val_acc,\n                \"best_so_far\": best_so_far,\n                \"cumulative_time\": cumulative_time,\n                \"wall_clock\": wall,\n            })\n\n    # summary\n    auc = _area_under_curve(np.arange(len(y_u)), y_u)\n    time_to_85 = _first_time_reach(np.array(y_u), np.array([cumulative_time]))\n    if cfg.wandb.mode != \"disabled\":\n        wandb.summary[\"final_best_val_acc\"] = best_so_far\n        wandb.summary[\"auc_accuracy\"] = auc\n        wandb.summary[\"time_to_85\"] = time_to_85\n\n\n# --------------------------------------------------------------------------------------\n# Utility functions\n# --------------------------------------------------------------------------------------\n\ndef _expected_improvement(mu, sigma, y_best):\n    \"\"\"Compute Expected Improvement.\"\"\"\n    from scipy.stats import norm\n    sigma = np.maximum(sigma, 1e-9)\n    z = (mu - y_best) / sigma\n    ei = (mu - y_best) * norm.cdf(z) + sigma * norm.pdf(z)\n    return ei\n\n\ndef _area_under_curve(xs, ys):\n    return np.trapz(ys, xs)\n\n\ndef _first_time_reach(acc_list, time_stamps, threshold=0.85):\n    for acc, t in zip(acc_list, time_stamps):\n        if acc \u003e= threshold:\n            return t\n    return float(\u0027inf\u0027)\n\n\n# --------------------------------------------------------------------------------------\n# Main entry\n# --------------------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # Adjust according to trial_mode\n    if cfg.trial_mode:\n        cfg.wandb.mode = \"disabled\"\n        cfg.training.epochs = 1\n        cfg.algorithm.total_evaluations = 1\n        cfg.optuna.n_trials = 0\n\n    # WandB init\n    if cfg.wandb.mode != \"disabled\":\n        wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        print(f\"WandB run URL: {wandb.run.get_url()}\")\n    else:\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n\n    # data loaders\n    device = _make_device(cfg)\n    train_loader, val_loader, _ = get_dataloaders(cfg)\n    loaders = (train_loader, val_loader)\n\n    if cfg.algorithm.name.lower() == \"boil\":\n        run_baseline_boil(cfg, loaders, device)\n    elif cfg.algorithm.name.lower() == \"ca-boil\":\n        run_ca_boil(cfg, loaders, device)\n    else:\n        raise ValueError(f\"Unknown algorithm {cfg.algorithm.name}\")\n\n    if cfg.wandb.mode != \"disabled\":\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n"}

Analyze the experiment code thoroughly. Ensure it is complete, executable, and ready for publication-quality research experiments.
Output:
{
    "is_code_ready": false,
    "code_issue": "Does not satisfy Core Validation Criteria:\n1. WandB integration incomplete – logs only once per hyper-parameter evaluation. Must log metrics at each training batch/epoch and confusion matrix; requirement 6.\n2. Dataset caching violates requirement 3 – run configs set dataset.root ./data; pipeline must force .cache/ for all datasets/models.\n3. WandB logging frequency and metric coverage insufficient (no training loss, per-batch/epoch logs, missing confusion matrix in summary).\n4. `train.py` ignores `results_dir` argument; requirement 2 expects use.\n5. Confusion matrix figure optional but requirement states it must always be generated; code skips if not present.\nTherefore code not production-ready."
}
