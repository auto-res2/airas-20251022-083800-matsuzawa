
LLM Name: o3-2025-04-16
Input:
You are a cutting-edge AI researcher generating complete, executable code for research paper experiments with Hydra configuration management.

Based on the research method in # Current Research Method and experimental design in # Experimental Design, generate production-ready experiment code that integrates with Hydra for configuration management.

# Instructions: Complete Experiment Code Generation

## Core Requirements
- COMPLETE IMPLEMENTATION: Every component must be fully functional, production-ready, publication-worthy code. No "omitted for brevity", no "simplified version", no TODO, PLACEHOLDER, pass, or ...
- PYTORCH EXCLUSIVELY: Use PyTorch as the deep learning framework
- HYDRA INTEGRATION: Use Hydra to manage all experiment configurations from `config/run/*.yaml` files. Use `config_path="../config"` in all @hydra.main decorators
- COMPLETE DATA PIPELINE: Full data loading and preprocessing implementation. Use `.cache/` as the cache directory for all datasets and models (e.g., for HuggingFace, set `cache_dir=".cache/"`)
- WANDB REQUIRED: WandB is mandatory for metrics logging (except trial_mode validation)

## Hydra Configuration Structure
Each run config file (`config/run/{run_id}.yaml`) contains:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings, validation split)
- optuna: Hyperparameter search space definition for Optuna optimization

## Command Line Interface
The generated code must support the following CLI:

**Training (main.py):**
```bash
# Full experiment with WandB logging
uv run python -u -m src.main run={run_id} results_dir={path}

# Trial mode (validation only, WandB disabled)
uv run python -u -m src.main run={run_id} results_dir={path} trial_mode=true
```
- `run`: Experiment run_id (matching a run_id from config/run/*.yaml)
- `results_dir`: Output directory (passed from GitHub Actions workflow)
- `trial_mode=true` (optional): Lightweight execution for validation (epochs=1, batches limited to 1-2, disable Optuna n_trials=0, **WandB disabled**)

**Evaluation (evaluate.py, independent execution):**
```bash
uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2", ...]'
```
- `results_dir`: Directory containing experiment metadata and where outputs will be saved
- `run_ids`: JSON string list of run IDs to evaluate (e.g., '["run-1-proposed-bert-glue", "run-2-baseline-bert-glue"]')
- Executed as a separate workflow after all training runs complete
- **NOT called from main.py**

## Script Structure (ExperimentCode format)
Generate complete code for these files ONLY. Do not create any additional files beyond this structure:

**`src/train.py`**: Single experiment run executor
- Uses Hydra config to load all parameters
- Called as subprocess by main.py
- Responsibilities:
  * Train model with given configuration
  * Initialize WandB: `wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project, id=cfg.run.run_id, config=OmegaConf.to_container(cfg, resolve=True), resume="allow")`
  * Skip `wandb.init()` if `cfg.wandb.mode == "disabled"` (trial_mode)
  * **Log ALL metrics to WandB comprehensively**:
    - Use `wandb.log()` at each training step/batch/epoch with ALL relevant metrics
    - Log as frequently as possible (per-batch or per-epoch) to capture training dynamics
  * **Save final/best metrics to WandB summary**:
    - Use `wandb.summary["key"] = value` for final results
  * Print WandB run URL to stdout
- **NO results.json, no stdout JSON output, no figure generation**

**`src/evaluate.py`**: Independent evaluation and visualization script
- **Execution**: Run independently via `uv run python -m src.evaluate results_dir={path} run_ids='["run-1", "run-2"]'`
- **NOT called from main.py** - executes as separate workflow after all training completes
- **Responsibilities**:
  * Parse command line arguments:
    - `results_dir`: Output directory path
    - `run_ids`: JSON string list of run IDs (parse with `json.loads(args.run_ids)`)
  * Load WandB config from `{results_dir}/config.yaml`
  * **Retrieve comprehensive experimental data from WandB API** for specified run_ids:
    ```python
    import json
    api = wandb.Api()
    run_ids = json.loads(args.run_ids)  # Parse JSON string to list
    for run_id in run_ids:
        run = api.run(f"{entity}/{project}/{run_id}")
        history = run.history()  # pandas DataFrame with ALL time-series metrics (train_loss, val_acc, etc.)
        summary = run.summary._json_dict  # Final/best metrics (best_val_acc, final_test_acc, etc.)
        config = dict(run.config)  # Run configuration (hyperparameters, model settings, etc.)
    ```
  * **STEP 1: Per-Run Processing** (for each run_id):
    - Export **comprehensive** run-specific metrics to: `{results_dir}/{run_id}/metrics.json`
    - Generate run-specific figures (learning curves, confusion matrices) to: `{results_dir}/{run_id}/`
    - Each run should have its own subdirectory with its metrics and figures
  * **STEP 2: Aggregated Analysis** (after processing all runs):
    - Export aggregated metrics to: `{results_dir}/comparison/aggregated_metrics.json`
    - Compute secondary/derived metrics (e.g., improvement rate: (proposed - baseline) / baseline)
    - Generate comparison figures to: `{results_dir}/comparison/`:
      * Cross-run comparison charts (bar charts, box plots)
      * Performance metrics tables
      * Statistical significance tests
  * **Figure Generation Guidelines**:
    - Use matplotlib or seaborn with proper legends, annotations, tight_layout
    - For line graphs: annotate significant values (final/best values)
    - For bar graphs: annotate values above each bar
    - Use GLOBALLY UNIQUE image filenames to prevent collisions across different runs and directories**:
      * Per-run figures: `{run_id}_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `run-1-proposed-bert-glue_learning_curve.pdf`)
      * Comparison figures: `comparison_{figure_topic}[_<condition>][_pairN].pdf` (e.g., `comparison_accuracy_bar_chart.pdf`)
  * Print all generated file paths to stdout (both per-run and comparison)

**`src/preprocess.py`**: Complete preprocessing pipeline implementation for the specified datasets

**`src/model.py`**: Complete model architecture implementations for all methods (proposed and comparative methods)

**`src/main.py`**: Main orchestrator
- Receives run_id via Hydra, launches train.py as subprocess, manages logs
- **DOES NOT call evaluate.py** (evaluate.py runs independently in separate workflow)
- Use `@hydra.main(config_path="../config")` since execution is from repository root
- Pass all Hydra overrides to train.py subprocess (e.g., `wandb.mode=disabled`, `trial_mode=true`)
- In trial_mode, automatically set `wandb.mode=disabled`

**`config/config.yaml`**: Main Hydra configuration file
- MUST include WandB configuration:
  ```yaml
  wandb:
    entity: gengaru617-personal
    project: 251021-test
    mode: online  # Automatically set to "disabled" in trial_mode
  ```

**`pyproject.toml`**: Complete project dependencies
- MUST include: `hydra-core`, `wandb` (required)
- Include as needed: `optuna`, `torch`, `transformers`, `datasets`, etc.


## Key Implementation Focus Areas
1. **Hydra-Driven Configuration**: All parameters loaded from run configs dynamically
2. **Algorithm Core**: Full implementation of the proposed method with proper abstraction
3. **Trial Mode Behavior**: trial_mode=true automatically disables WandB (sets wandb.mode=disabled)
4. **Run Execution**: main.py executes a single run_id passed via CLI (GitHub Actions dispatches multiple runs separately)
5. **WandB Integration**: All metrics logged to WandB; train.py does NOT output JSON to stdout or save results.json
6. **Independent Evaluation**: evaluate.py runs separately, fetches data from WandB API, generates all figures



## Code Validation Feedback




# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Current Research Method
{
    "Open Problems": "1. BOIL estimates the wall-clock cost of evaluating a hyper-parameter setting with a plain LinearRegression model.  In practice the relation (e.g. GPU time, number of simulator steps) is highly non-linear and heteroscedastic.\n2. The acquisition function divides the expected utility by the point estimate of the cost, ignoring its uncertainty; when the cost model is wrong the optimiser can select apparently cheap but actually expensive configurations, wasting budget.\n3. Both shortcomings can be fixed with a very small change – replacing the linear cost surrogate with a probabilistic model and letting the acquisition treat cost uncertainty explicitly.",
    "Methods": "Method name: CA-BOIL (Cost-Aware BOIL)\nMinimal changes to BOIL:\n1. Replace the LinearRegression cost model with a second Gaussian-Process surrogate GP_c(x,t) that yields mean µ_c and standard deviation σ_c of the (log) cost.\n2. Redefine the acquisition used in utility_cost_evaluation from\n        U_old =  log(EI) – log(µ_c)\n   to the cost-uncertainty–aware\n        U_new =  EI /( µ_c + κ · σ_c )\n   (equivalently  log(EI) – log(µ_c+κσ_c) in BOIL’s code).  κ≥0 is a single scalar (default κ=1).\nTheoretical intuition:\n• GP_c gives a calibrated posterior over cost.  Dividing by µ_c+κσ_c is the familiar concept of \"expected improvement per (risk-adjusted) cost\"; high uncertainty on cost is penalised so the optimiser prefers configurations that are both promising and reliably cheap.\n• The change keeps BOIL’s product-kernel GP for utility, the learning-curve compression, the data-augmentation, etc. completely intact – only 25 lines of code are touched.",
    "Experimental Setup": "Goal: show that CA-BOIL reaches the same test accuracy with less wall-clock time than vanilla BOIL.\nTask: tune (learning-rate, hidden-units) of an MLP on the Fashion-MNIST dataset for 10 epochs.\nTrue cost: measured GPU time per evaluation (simulated here as  0.4·hidden_units + 1200·learning_rate^2  + noise).\nProtocol:\n1. Use the public BOIL implementation as the baseline (n_init_points=3, budget 40 evaluations).\n2. Implement CA-BOIL by adding GP_c and U_new (κ=1).\n3. Run each optimiser 20 times with different random seeds.\nMetrics:\n• Best validation accuracy reached vs. cumulative true time (area under curve).\n• Total time to reach 85% accuracy.",
    "Experimental Code": "import numpy as np, time, torch, sklearn\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel\n\n# === tiny wrapper around the user objective =================================\n\ndef objective(cfg):\n    lr, h = cfg  # learning-rate, hidden units\n    start = time.time()\n    # --- train a very small network for 10 epochs (placeholder) ------------\n    acc = 1.0 - (lr*3 + (200-h)/400) + np.random.randn()*0.01  # fake accuracy\n    true_cost = 0.4*h + 1200*lr**2 + np.random.rand()*3        # fake time\n    return acc, np.log(true_cost), time.time()-start\n\n# === cost-aware acquisition =================================================\n\ndef expected_improvement(mu, sigma, y_best):\n    from scipy.stats import norm\n    z = (mu - y_best)/sigma\n    return (mu - y_best)*norm.cdf(z) + sigma*norm.pdf(z)\n\ndef acquisition(X, gp_u, gp_c, y_best, kappa):\n    mu_u, sigma_u = gp_u.predict(X, return_std=True)\n    mu_c, sigma_c = gp_c.predict(X, return_std=True)\n    ei = expected_improvement(mu_u, sigma_u, y_best)\n    return ei / (mu_c + kappa*sigma_c)\n\n# === minimal CA-BOIL optimisation loop (no virtual obs for brevity) =========\n\nbounds = np.array([[1e-4, 1e-1],   # learning rate\n                   [ 50 , 400 ]])  # hidden units\n\ngp_u = GaussianProcessRegressor(Matern()+WhiteKernel(1e-5))\ngp_c = GaussianProcessRegressor(Matern()+WhiteKernel(1e-5))\n\nX, y_u, y_c = [], [], []\n# initial design\nfor _ in range(3):\n    x = np.random.uniform(bounds[:,0], bounds[:,1])\n    acc, cost_log, _ = objective(x)\n    X.append(x)\n    y_u.append(acc)\n    y_c.append(cost_log)\n\nfor it in range(37):  # total 40\n    gp_u.fit(np.array(X), np.array(y_u))\n    gp_c.fit(np.array(X), np.array(y_c))\n\n    y_best = np.max(y_u)\n    # optimise acquisition by random search (sufficient for demo)\n    cand = np.random.uniform(bounds[:,0], bounds[:,1], size=(5000,2))\n    acq_values = acquisition(cand, gp_u, gp_c, y_best, kappa=1.0)\n    x_next = cand[np.argmax(acq_values)]\n\n    acc, cost_log, _ = objective(x_next)\n    X.append(x_next)\n    y_u.append(acc)\n    y_c.append(cost_log)\n\nprint('best val acc', np.max(y_u))\n",
    "Expected Result": "Across 20 independent runs CA-BOIL reduces the median wall-clock time to reach 85% accuracy by ≈20-30% compared with BOIL, and achieves slightly higher final accuracy for the same fixed time budget because it avoids a few very costly evaluations that BOIL selects due to under-estimated cost.",
    "Expected Conclusion": "By swapping the linear cost surrogate for a simple GP and dividing the improvement by a risk-adjusted cost estimate, CA-BOIL becomes less prone to costly mistakes while keeping BOIL’s strengths (learning-curve compression and data augmentation).  The single added κ hyper-parameter and ~25 lines of code yield a consistent speed-up, demonstrating that even very small, principled modifications can translate into tangible efficiency gains in hyper-parameter optimisation."
}

# Experimental Design
- Summary: We benchmark the new Cost-Aware BOIL (CA-BOIL) hyper-parameter optimiser against the original BOIL on a medium-size neural-network tuning task.  The optimiser must find the learning-rate (10⁻⁴–10⁻¹) and hidden-layer width (50–400 units) that maximise validation accuracy of a 2-layer MLP after ten training epochs on Fashion-MNIST while paying the true wall-clock cost of each trial (measured GPU time).  
Workflow
1. Dataset loading: Fashion-MNIST train/validation split.
2. Model definition: a fixed 2-layer MLP (≈0.1 M parameters) implemented in PyTorch.
3. Objective function: (cfg) ↦ (validation-accuracy, log(cost)).  Cost is obtained through torch.cuda.Event timers.
4. Optimisers:
   • BOIL baseline: single GP for utility, linear regression for cost, acquisition U_old = log(EI) – log(µ_c).
   • Proposed CA-BOIL: identical utility GP but replaces the linear cost model with a GP GP_c and uses acquisition U_new = EI / (µ_c + κ σ_c) (κ = 1).
5. Loop: 3 Sobol initial points + 37 adaptive steps (budget 40).  Both methods are repeated for 20 random seeds on the same machine (8 × A100, 80 GB each).
6. Logging: after every evaluation we record cumulative wall-clock time, best validation accuracy, chosen hyper-parameters, and acquisition value.
7. Post-processing: compute per-run curves best-acc(t), integrate AUC, and measure the time when 85 % validation accuracy is first reached.
8. Statistical analysis: report median and 95 %-CI across seeds, and perform two-sided Wilcoxon signed-rank tests on each metric.
The experiment demonstrates that CA-BOIL reaches 85 % accuracy ≈25 % faster and avoids extreme outliers in time consumption, validating that modelling cost uncertainty yields more budget-robust optimisation.
- Evaluation metrics: ['Area Under Best-Accuracy–Time Curve', 'Time To 85% Accuracy', 'Final Best Validation Accuracy']

# Experiment Runs

- Run ID: proposed-2-Layer-MLP-0.1M-parameters--Fashion-MNIST
  Method: proposed
  Model: 2-Layer MLP (≈0.1M parameters)
  Dataset: Fashion-MNIST
  Config File: config/run/proposed-2-Layer-MLP-0.1M-parameters--Fashion-MNIST.yaml
  
  Config Content:
  ```yaml
  run_id: proposed-2-Layer-MLP-0.1M-parameters--Fashion-MNIST
method: proposed
algorithm:
  name: CA-BOIL
  n_init_points: 3         # Sobol seed evaluations
  total_evaluations: 40    # 3 initial + 37 BO steps
  cost_model:
    type: gp
    kernel: matern_plus_white
  acquisition:
    type: ei_over_risk_adjusted_cost
    kappa: 1.0             # default; can be tuned by Optuna
    formula: "EI / (mu_c + kappa * sigma_c)"
model:
  name: 2-layer-mlp
  input_dim: 784           # 28×28 images flattened
  hidden_units: 200        # placeholder, will be overridden by Optuna suggestion
  activation: relu
  output_dim: 10
  parameter_budget: "≈0.1M"
dataset:
  name: fashion_mnist
  root: ./data
  train_size: 50000        # 83 % of full train split
  val_size: 10000          # 17 % of full train split
  normalization: true
training:
  epochs: 10               # fixed according to protocol
  batch_size: 128
  optimizer: adam
  weight_decay: 0.0
  max_grad_norm: 1.0
  device: cuda
logging:
  use_wandb: true
  project: ca-boil
  log_interval: 1          # log after each evaluation
resources:
  gpus_per_trial: 1
  gpu_type: A100
  cpus_per_trial: 4
  memory_gb: 16
optuna:
  n_trials: 50
  direction: maximize
  sampler: tpe
  pruner: median
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    hidden_units:
      type: int
      low: 50
      high: 400
      step: 1
    kappa:
      type: uniform
      low: 0.5
      high: 2.0

  ```
  

- Run ID: comparative-1-2-Layer-MLP-0.1M-parameters--Fashion-MNIST
  Method: comparative-1
  Model: 2-Layer MLP (≈0.1M parameters)
  Dataset: Fashion-MNIST
  Config File: config/run/comparative-1-2-Layer-MLP-0.1M-parameters--Fashion-MNIST.yaml
  
  Config Content:
  ```yaml
  run_id: comparative-1-2-Layer-MLP-0.1M-parameters--Fashion-MNIST
method: comparative-1
algorithm:
  name: BOIL
  n_init_points: 3
  total_evaluations: 40
  cost_model:
    type: linear           # original BOIL uses plain LinearRegression
  acquisition:
    type: log_ei_minus_log_cost
    formula: "log(EI) - log(mu_c)"
model:
  name: 2-layer-mlp
  input_dim: 784
  hidden_units: 200        # placeholder, tuned by Optuna
  activation: relu
  output_dim: 10
  parameter_budget: "≈0.1M"
dataset:
  name: fashion_mnist
  root: ./data
  train_size: 50000
  val_size: 10000
  normalization: true
training:
  epochs: 10
  batch_size: 128
  optimizer: adam
  weight_decay: 0.0
  max_grad_norm: 1.0
  device: cuda
logging:
  use_wandb: true
  project: boil-baseline
  log_interval: 1
resources:
  gpus_per_trial: 1
  gpu_type: A100
  cpus_per_trial: 4
  memory_gb: 16
optuna:
  n_trials: 50
  direction: maximize
  sampler: tpe
  pruner: median
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-4
      high: 1e-1
    hidden_units:
      type: int
      low: 50
      high: 400
      step: 1

  ```
  


# External Resources (Use these for implementation)

**HuggingFace Models:**


**HuggingFace Datasets:**

- ID: zalando-datasets/fashion_mnist




Generate complete, production-ready experiment code that integrates with Hydra configuration system.
Output:
{
    "train_py": "import os\nimport sys\nimport time\nimport json\nimport math\nimport random\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig, OmegaConf\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern, WhiteKernel\nfrom sklearn.linear_model import LinearRegression\nimport wandb\nfrom tqdm import tqdm\n\nfrom src.preprocess import get_dataloaders\nfrom src.model import MultiLayerPerceptron\n\n# --------------------------------------------------------------------------------------\n# Helper functions\n# --------------------------------------------------------------------------------------\n\ndef _make_device(cfg):\n    if torch.cuda.is_available() and cfg.training.device == \"cuda\":\n        return torch.device(\"cuda\")\n    return torch.device(\"cpu\")\n\n\ndef _time_now():\n    return time.perf_counter()\n\n\ndef _measure_runtime(fn):\n    \"\"\"Decorator to measure wall-clock time of fn.\"\"\"\n    def wrapper(*args, **kwargs):\n        start = _time_now()\n        out = fn(*args, **kwargs)\n        cost = _time_now() - start\n        return out, cost\n    return wrapper\n\n\n@_measure_runtime\ndef _train_one_setting(lr: float, hidden_units: int, cfg: DictConfig, loaders, device):\n    train_loader, val_loader = loaders\n    model = MultiLayerPerceptron(\n        input_dim=cfg.model.input_dim,\n        hidden_units=hidden_units,\n        output_dim=cfg.model.output_dim,\n        activation=cfg.model.activation,\n    ).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=cfg.training.weight_decay)\n\n    best_acc = 0.0\n    model.train()\n    for epoch in range(cfg.training.epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            if cfg.trial_mode and batch_idx >= 2:\n                break  # drastically shorten trial mode\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n        # validation after each epoch (simple)\n        val_acc = _evaluate(model, val_loader, device, cfg.trial_mode)\n        best_acc = max(best_acc, val_acc)\n    # confusion matrix at the end using best model (last epoch here)\n    cm = _confusion_matrix(model, val_loader, device, cfg.model.output_dim, cfg.trial_mode)\n    return {\n        \"best_val_accuracy\": best_acc,\n        \"confusion_matrix\": cm,\n        \"final_model_state_dict\": model.state_dict(),\n    }\n\n\ndef _evaluate(model, loader, device, trial_mode=False):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(loader):\n            if trial_mode and batch_idx >= 2:\n                break\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    return correct / total if total > 0 else 0.0\n\n\ndef _confusion_matrix(model, loader, device, num_classes, trial_mode=False):\n    all_pred, all_true = [], []\n    model.eval()\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(loader):\n            if trial_mode and batch_idx >= 5:\n                break\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            _, preds = outputs.max(1)\n            all_pred.extend(preds.cpu().numpy())\n            all_true.extend(targets.numpy())\n    cm = confusion_matrix(all_true, all_pred, labels=list(range(num_classes)))\n    return cm.tolist()\n\n\n# --------------------------------------------------------------------------------------\n# BOIL baseline\n# --------------------------------------------------------------------------------------\n\ndef run_baseline_boil(cfg, loaders, device):\n    bounds = np.array([[math.log10(cfg.optuna.search_space.learning_rate.low),\n                        math.log10(cfg.optuna.search_space.learning_rate.high)],\n                       [cfg.optuna.search_space.hidden_units.low,\n                        cfg.optuna.search_space.hidden_units.high]])\n\n    def _sample_random(n):\n        lr = 10 ** np.random.uniform(bounds[0, 0], bounds[0, 1], size=n)\n        hidden = np.random.randint(bounds[1, 0], bounds[1, 1] + 1, size=n)\n        return np.stack([lr, hidden], axis=1)\n\n    n_init = cfg.algorithm.n_init_points\n    total_evals = cfg.algorithm.total_evaluations if not cfg.trial_mode else 1\n\n    X, y_u, y_c = [], [], []\n    cumulative_time = 0.0\n    best_so_far = 0.0\n\n    cost_model = LinearRegression()\n    utility_gp = GaussianProcessRegressor(kernel=Matern() + WhiteKernel(1e-5))\n\n    for evaluation in range(total_evals):\n        if evaluation < n_init:\n            x_next = _sample_random(1)[0]\n        else:\n            # fit models\n            utility_gp.fit(np.array(X), np.array(y_u))\n            cost_model.fit(np.array(X), np.array(y_c))\n            mu_c = cost_model.predict\n\n            # random search for acquisition\n            cand = _sample_random(4000)\n            mu_u, sigma_u = utility_gp.predict(cand, return_std=True)\n            cost_pred = mu_c(cand)\n            ei = _expected_improvement(mu_u, sigma_u, max(y_u))\n            acq = np.log(ei + 1e-12) - np.log(np.exp(cost_pred))  # since cost is log-cost\n            x_next = cand[np.argmax(acq)]\n\n        # evaluate objective\n        lr, hidden = float(x_next[0]), int(round(x_next[1]))\n        res, wall = _train_one_setting(lr, hidden, cfg, loaders, device)\n        val_acc = res[\"best_val_accuracy\"]\n\n        cumulative_time += wall\n        X.append(x_next)\n        y_u.append(val_acc)\n        y_c.append(np.log(wall))  # use log-cost as in BOIL paper\n        best_so_far = max(best_so_far, val_acc)\n\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log({\n                \"evaluation\": evaluation,\n                \"lr\": lr,\n                \"hidden_units\": hidden,\n                \"val_accuracy\": val_acc,\n                \"best_so_far\": best_so_far,\n                \"cumulative_time\": cumulative_time,\n                \"wall_clock\": wall,\n            })\n\n    # summary\n    auc = _area_under_curve(np.arange(len(y_u)), y_u)\n    time_to_85 = _first_time_reach(np.array(y_u), np.array([cumulative_time]))\n    if cfg.wandb.mode != \"disabled\":\n        wandb.summary[\"final_best_val_acc\"] = best_so_far\n        wandb.summary[\"auc_accuracy\"] = auc\n        wandb.summary[\"time_to_85\"] = time_to_85\n\n\n# --------------------------------------------------------------------------------------\n# CA-BOIL proposed method\n# --------------------------------------------------------------------------------------\n\ndef run_ca_boil(cfg, loaders, device):\n    bounds = np.array([[math.log10(cfg.optuna.search_space.learning_rate.low),\n                        math.log10(cfg.optuna.search_space.learning_rate.high)],\n                       [cfg.optuna.search_space.hidden_units.low,\n                        cfg.optuna.search_space.hidden_units.high]])\n\n    def _sample_random(n):\n        lr = 10 ** np.random.uniform(bounds[0, 0], bounds[0, 1], size=n)\n        hidden = np.random.randint(bounds[1, 0], bounds[1, 1] + 1, size=n)\n        return np.stack([lr, hidden], axis=1)\n\n    n_init = cfg.algorithm.n_init_points\n    total_evals = cfg.algorithm.total_evaluations if not cfg.trial_mode else 1\n    kappa = cfg.algorithm.acquisition.kappa\n\n    X, y_u, y_c = [], [], []\n    cumulative_time = 0.0\n    best_so_far = 0.0\n\n    utility_gp = GaussianProcessRegressor(kernel=Matern() + WhiteKernel(1e-5))\n    cost_gp = GaussianProcessRegressor(kernel=Matern() + WhiteKernel(1e-5))\n\n    for evaluation in range(total_evals):\n        if evaluation < n_init:\n            x_next = _sample_random(1)[0]\n        else:\n            utility_gp.fit(np.array(X), np.array(y_u))\n            cost_gp.fit(np.array(X), np.array(y_c))\n\n            cand = _sample_random(4000)\n            mu_u, sigma_u = utility_gp.predict(cand, return_std=True)\n            mu_c, sigma_c = cost_gp.predict(cand, return_std=True)\n\n            ei = _expected_improvement(mu_u, sigma_u, max(y_u))\n            denom = mu_c + kappa * sigma_c\n            acq = ei / (denom + 1e-9)\n            x_next = cand[np.argmax(acq)]\n\n        lr, hidden = float(x_next[0]), int(round(x_next[1]))\n        res, wall = _train_one_setting(lr, hidden, cfg, loaders, device)\n        val_acc = res[\"best_val_accuracy\"]\n\n        cumulative_time += wall\n        X.append(x_next)\n        y_u.append(val_acc)\n        y_c.append(np.log(wall))\n        best_so_far = max(best_so_far, val_acc)\n\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log({\n                \"evaluation\": evaluation,\n                \"lr\": lr,\n                \"hidden_units\": hidden,\n                \"val_accuracy\": val_acc,\n                \"best_so_far\": best_so_far,\n                \"cumulative_time\": cumulative_time,\n                \"wall_clock\": wall,\n            })\n\n    # summary\n    auc = _area_under_curve(np.arange(len(y_u)), y_u)\n    time_to_85 = _first_time_reach(np.array(y_u), np.array([cumulative_time]))\n    if cfg.wandb.mode != \"disabled\":\n        wandb.summary[\"final_best_val_acc\"] = best_so_far\n        wandb.summary[\"auc_accuracy\"] = auc\n        wandb.summary[\"time_to_85\"] = time_to_85\n\n\n# --------------------------------------------------------------------------------------\n# Utility functions\n# --------------------------------------------------------------------------------------\n\ndef _expected_improvement(mu, sigma, y_best):\n    \"\"\"Compute Expected Improvement.\"\"\"\n    from scipy.stats import norm\n    sigma = np.maximum(sigma, 1e-9)\n    z = (mu - y_best) / sigma\n    ei = (mu - y_best) * norm.cdf(z) + sigma * norm.pdf(z)\n    return ei\n\n\ndef _area_under_curve(xs, ys):\n    return np.trapz(ys, xs)\n\n\ndef _first_time_reach(acc_list, time_stamps, threshold=0.85):\n    for acc, t in zip(acc_list, time_stamps):\n        if acc >= threshold:\n            return t\n    return float('inf')\n\n\n# --------------------------------------------------------------------------------------\n# Main entry\n# --------------------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # Adjust according to trial_mode\n    if cfg.trial_mode:\n        cfg.wandb.mode = \"disabled\"\n        cfg.training.epochs = 1\n        cfg.algorithm.total_evaluations = 1\n        cfg.optuna.n_trials = 0\n\n    # WandB init\n    if cfg.wandb.mode != \"disabled\":\n        wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        print(f\"WandB run URL: {wandb.run.get_url()}\")\n    else:\n        os.environ[\"WANDB_MODE\"] = \"disabled\"\n\n    # data loaders\n    device = _make_device(cfg)\n    train_loader, val_loader, _ = get_dataloaders(cfg)\n    loaders = (train_loader, val_loader)\n\n    if cfg.algorithm.name.lower() == \"boil\":\n        run_baseline_boil(cfg, loaders, device)\n    elif cfg.algorithm.name.lower() == \"ca-boil\":\n        run_ca_boil(cfg, loaders, device)\n    else:\n        raise ValueError(f\"Unknown algorithm {cfg.algorithm.name}\")\n\n    if cfg.wandb.mode != \"disabled\":\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport wandb\nfrom scipy.stats import wilcoxon\n\n\n# --------------------------------------------------------------------------------------\n# Helper utilities\n# --------------------------------------------------------------------------------------\n\ndef _ensure_dir(p: Path):\n    p.mkdir(parents=True, exist_ok=True)\n    return p\n\n\ndef _plot_learning_curve(history_df: pd.DataFrame, run_id: str, out_path: Path):\n    plt.figure(figsize=(6, 4))\n    if \"cumulative_time\" in history_df.columns:\n        x = history_df[\"cumulative_time\"]\n    else:\n        x = history_df.index\n    y = history_df[\"best_so_far\"] if \"best_so_far\" in history_df.columns else history_df[\"val_accuracy\"]\n    plt.plot(x, y, label=\"Validation Accuracy\")\n    plt.xlabel(\"Cumulative Time (s)\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(f\"Learning Curve {run_id}\")\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _plot_confusion_matrix(cm: np.ndarray, run_id: str, out_path: Path):\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.title(f\"Confusion Matrix {run_id}\")\n    plt.tight_layout()\n    plt.savefig(out_path, format=\"pdf\")\n    plt.close()\n\n\ndef _aggregate_and_compare(results, out_dir: Path):\n    records = []\n    for run_id, data in results.items():\n        summary = data[\"summary\"]\n        records.append({\n            \"run_id\": run_id,\n            \"best_val_acc\": summary.get(\"final_best_val_acc\", float(\"nan\")),\n            \"auc\": summary.get(\"auc_accuracy\", float(\"nan\")),\n            \"time_to_85\": summary.get(\"time_to_85\", float(\"nan\")),\n        })\n    df = pd.DataFrame.from_records(records)\n    _ensure_dir(out_dir)\n    df.to_json(out_dir / \"aggregated_metrics.json\", orient=\"records\", lines=False, indent=2)\n\n    # Bar chart of final accuracy\n    plt.figure(figsize=(6, 4))\n    sns.barplot(x=\"run_id\", y=\"best_val_acc\", data=df)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Best Validation Accuracy\")\n    for i, v in enumerate(df[\"best_val_acc\"]):\n        plt.text(i, v + 0.001, f\"{v:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n    plt.tight_layout()\n    bar_path = out_dir / \"comparison_best_accuracy_bar_chart.pdf\"\n    plt.savefig(bar_path, format=\"pdf\")\n    plt.close()\n\n    # Statistical significance (wilcoxon) if we have pairs\n    if len(df) == 2:\n        try:\n            stat, p = wilcoxon(df.loc[0, \"best_val_acc\"], df.loc[1, \"best_val_acc\"])\n        except Exception:\n            p = float(\"nan\")\n        with open(out_dir / \"statistics.txt\", \"w\") as f:\n            f.write(f\"Wilcoxon p-value on best_val_acc: {p}\\n\")\n    print(f\"Generated aggregated metrics and comparison figure at {out_dir}\")\n\n\n# --------------------------------------------------------------------------------------\n# Main\n# --------------------------------------------------------------------------------------\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON string list of run IDs\")\n    args = parser.parse_args()\n\n    results_dir = Path(args.results_dir)\n    run_ids = json.loads(args.run_ids)\n\n    api = wandb.Api()\n\n    all_results = {}\n\n    for run_id in run_ids:\n        run_save_dir = _ensure_dir(results_dir / run_id)\n\n        # Retrieve run\n        run = api.run(f\"{os.environ.get('WANDB_ENTITY', 'gengaru617-personal')}/{os.environ.get('WANDB_PROJECT', '251021-test')}/{run_id}\")\n        history = run.history()  # pandas DataFrame\n        summary = run.summary._json_dict\n        config = dict(run.config)\n\n        # Export metrics\n        metrics_path = run_save_dir / \"metrics.json\"\n        history.to_json(metrics_path, orient=\"records\", lines=False, indent=2)\n        print(metrics_path)\n\n        # Learning curve figure\n        lc_path = run_save_dir / f\"{run_id}_learning_curve.pdf\"\n        _plot_learning_curve(history, run_id, lc_path)\n        print(lc_path)\n\n        # Confusion matrix figure (if available)\n        if \"confusion_matrix\" in summary:\n            cm_array = np.array(summary[\"confusion_matrix\"])\n            cm_path = run_save_dir / f\"{run_id}_confusion_matrix.pdf\"\n            _plot_confusion_matrix(cm_array, run_id, cm_path)\n            print(cm_path)\n\n        all_results[run_id] = {\n            \"history\": history,\n            \"summary\": summary,\n            \"config\": config,\n        }\n\n    # Aggregated analysis\n    comparison_dir = results_dir / \"comparison\"\n    _aggregate_and_compare(all_results, comparison_dir)\n    print(comparison_dir / \"aggregated_metrics.json\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "from typing import Tuple\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import FashionMNIST\n\n\nCACHE_DIR = Path(\".cache\")\n\n\ndef get_dataloaders(cfg) -> Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"Return train, val, test dataloaders as specified in cfg.dataset.\"\"\"\n    root = Path(cfg.dataset.root) if hasattr(cfg.dataset, \"root\") else CACHE_DIR / \"fashion_mnist\"\n    root.mkdir(parents=True, exist_ok=True)\n\n    transform_list = [transforms.ToTensor()]\n    if cfg.dataset.normalization:\n        transform_list.append(transforms.Normalize((0.5,), (0.5,)))\n    transform = transforms.Compose(transform_list)\n\n    full_train = FashionMNIST(root=str(root), train=True, transform=transform, download=True)\n    test_set = FashionMNIST(root=str(root), train=False, transform=transform, download=True)\n\n    train_size = cfg.dataset.train_size\n    val_size = cfg.dataset.val_size\n    assert train_size + val_size <= len(full_train), \"Train/val sizes exceed dataset length\"\n\n    train_subset, val_subset = random_split(full_train, [train_size, val_size])\n\n    batch_size = cfg.training.batch_size\n\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    return train_loader, val_loader, test_loader\n",
    "model_py": "import torch.nn as nn\n\n\nclass MultiLayerPerceptron(nn.Module):\n    def __init__(self, input_dim: int, hidden_units: int, output_dim: int, activation: str = \"relu\"):\n        super().__init__()\n        if activation == \"relu\":\n            act = nn.ReLU()\n        elif activation == \"tanh\":\n            act = nn.Tanh()\n        else:\n            raise ValueError(f\"Unsupported activation {activation}\")\n\n        self.net = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(input_dim, hidden_units),\n            act,\n            nn.Linear(hidden_units, output_dim),\n        )\n\n    def forward(self, x):  # type: ignore\n        return self.net(x)\n",
    "main_py": "import subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    \"\"\"Orchestrator that spawns src.train as a subprocess.\"\"\"\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run.run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n    ]\n\n    # propagate trial_mode\n    if cfg.trial_mode:\n        cmd.append(\"trial_mode=true\")\n        cmd.append(\"wandb.mode=disabled\")\n\n    print(\"Executing:\", \" \".join(cmd))\n\n    proc = subprocess.Popen(cmd)\n    proc.communicate()\n    if proc.returncode != 0:\n        raise SystemExit(proc.returncode)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[tool.poetry]\nname = \"ca-boil-experiments\"\nversion = \"0.1.0\"\ndescription = \"Cost-Aware BOIL experimental code\"\nauthors = [\"Researcher <research@example.com>\"]\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\nhydra-core = \"^1.3.2\"\nwandb = \"^0.16.0\"\ntorch = \"^2.1.0\"\ntorchvision = \"^0.16.0\"\nscikit-learn = \"^1.3.2\"\noptuna = \"^3.4.0\"\nnumpy = \"^1.26.2\"\nmatplotlib = \"^3.8.0\"\nseaborn = \"^0.13.0\"\npandas = \"^2.1.3\"\nscipy = \"^1.11.3\"\ntqdm = \"^4.66.1\"\nomegaconf = \"^2.3.0\"\n",
    "config_yaml": "defaults:\n  - _self_\n  - run: ???\n\n# --- general flags -----------------------------------------------------------\nrun:\n  run_id: null\n\nresults_dir: ./results\ntrial_mode: false\n\n# --- wandb -------------------------------------------------------------------\nwandb:\n  entity: gengaru617-personal\n  project: 251021-test\n  mode: online\n\n# --- placeholders that run-configs overwrite ---------------------------------\nmethod: \"\"\nalgorithm:\n  name: \"\"\n  n_init_points: 3\n  total_evaluations: 40\n  cost_model: {}\n  acquisition: {}\nmodel: {}\ndataset: {}\ntraining:\n  device: cuda\n  epochs: 10\n  batch_size: 128\n  optimizer: adam\n  weight_decay: 0.0\n  max_grad_norm: 1.0\n\noptuna:\n  n_trials: 50\n  direction: maximize\n  sampler: tpe\n  pruner: median\n  search_space: {}\n"
}
